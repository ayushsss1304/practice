from tensorflow.keras import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.datasets import imdb
from tensorflow.keras.layers import Dense,Embedding,LSTM,Bidirectional
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.sequence import pad_sequences

vocab_size=10000
max_length=200
(X_train,y_train),(X_test,y_test)=imdb.load_data(num_words=vocab_size)
X_train=pad_sequences(X_train, maxlen=max_length)
X_test=pad_sequences(X_test, maxlen=max_length)

model=Sequential([
    Embedding(vocab_size,8),
    Bidirectional(LSTM(8,return_sequences=False)),
    Dense(8,activation='relu'),
    Dense(1,activation='sigmoid')
])

model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])
history=model.fit(X_train,y_train,epochs=2,batch_size=32,validation_split=0.2)

model.evaluate(X_test,y_test)

plt.plot(history.history['accuracy'],label='Train Acc')
plt.plot(history.history['val_accuracy'],label='Val Acc')
plt.legend()
plt.show()
